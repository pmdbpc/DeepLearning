{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Dependencies Completed.\n"
     ]
    }
   ],
   "source": [
    "# Importing all dependent APIs\n",
    "import os, cv2, random, time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "print(\"Importing Dependencies Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pass images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass: 100%|█████████████████████████████████████████████████████████████████████▉| 12815/12821 [03:12<00:00, 70.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Fail images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fail: 100%|█████████████████████████████████████████████████████████████████████▉| 12818/12821 [03:07<00:00, 69.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training data:  25642\n",
      "(24000, 256, 256, 1)\n",
      "(1642, 256, 256, 1)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Creating a database for running CNN\n",
    "dataDir = r\"E:\\DeepLearning\\ImageDatasets\\Dataset01-EBI-McD180mlBottom\\EBIResult\\Images\"\n",
    "categories = [\"Pass\", \"Fail\"]\n",
    "trainingData = []\n",
    "xTrain = []\n",
    "yTrain = []\n",
    "xTest = []\n",
    "yTest = []\n",
    "imgSizeC = 256\n",
    "\n",
    "def createTrainingData():\n",
    "    for category in categories:\n",
    "        path = os.path.join(dataDir, category)    # path to cats or dogs dir\n",
    "        print(\"Loading \" + category + \" images.\")\n",
    "        time.sleep(0.25)\n",
    "        classNum = categories.index(category)\n",
    "        loop = tqdm(total=len(os.listdir(path)),desc=category, position=0, leave=False)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                imgArray = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                cropped = imgArray[:, 88:1112]\n",
    "                newArrC = cv2.resize(cropped, (imgSizeC, imgSizeC))\n",
    "                trainingData.append([newArrC, classNum])\n",
    "                loop.update()\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    random.shuffle(trainingData)\n",
    "\n",
    "createTrainingData()\n",
    "\n",
    "print(\"Length of training data: \",len(trainingData))\n",
    "\n",
    "for featuresTrain, labelTrain in trainingData[:24000]:\n",
    "    xTrain.append(featuresTrain)\n",
    "    yTrain.append(labelTrain)\n",
    "    \n",
    "for featuresTest, labelTest in trainingData[24000:]:\n",
    "    xTest.append(featuresTest)\n",
    "    yTest.append(labelTest)\n",
    "\n",
    "xTrain = np.array(xTrain).reshape(-1, imgSizeC, imgSizeC, 1)\n",
    "xTest = np.array(xTest).reshape(-1, imgSizeC, imgSizeC, 1)\n",
    "\n",
    "print(xTrain.shape)\n",
    "print(xTest.shape)\n",
    "\n",
    "#print(yTrain.shape)\n",
    "#print(yTest.shape)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Fail: 100%|██████████████████████████████████████████████████████████████████████| 12821/12821 [03:26<00:00, 69.69it/s]"
     ]
    }
   ],
   "source": [
    "# Pre-processing stage\n",
    "# Reshaping loaded image dataset to 3 dimensions and plotting sample image.\n",
    "#print(xTrain.shape) # 200, 256, 256, 1\n",
    "#xTrain = xTrain.astype('float32')\n",
    "xTrain = np.array(xTrain, dtype=np.float32)\n",
    "#xTest = xTest.astype('float32')\n",
    "xTest = np.array(xTest, dtype=np.float32)\n",
    "xTrain /= 255\n",
    "xTest /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "[1, 0, 1, 0, 0, 0, 1, 0, 1, 1]\n",
      "========================\n",
      "(24000, 2)\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(len(yTrain))\n",
    "print(yTrain[:10])\n",
    "# Train & Test datasets are represented as single array with class values.\n",
    "\n",
    "# Splitting Train & Test datasets in 2 distinct class labels\n",
    "# Convert 1-D Class arrays to 2-D Class matrices\n",
    "yTrain = np_utils.to_categorical(yTrain,2)\n",
    "yTest = np_utils.to_categorical(yTest,2)\n",
    "print(\"========================\")\n",
    "print(yTrain.shape)\n",
    "print(yTrain[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 254, 254, 32)\n"
     ]
    }
   ],
   "source": [
    "# Declaring sequential model format\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32,(3,3),activation='relu',input_shape=(256,256,1)))\n",
    "print(model.output_shape)\n",
    "# (None, 254, 254, 32)\n",
    "\n",
    "model.add(Convolution2D(32,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model compilation\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24000/24000 [==============================] - 1754s 73ms/step - loss: 0.4258 - acc: 0.8906\n",
      "Epoch 2/10\n",
      "24000/24000 [==============================] - 1908s 80ms/step - loss: 0.0818 - acc: 0.9745\n",
      "Epoch 3/10\n",
      "24000/24000 [==============================] - 1798s 75ms/step - loss: 0.0400 - acc: 0.9880\n",
      "Epoch 4/10\n",
      "24000/24000 [==============================] - 1849s 77ms/step - loss: 0.0296 - acc: 0.9913\n",
      "Epoch 5/10\n",
      "24000/24000 [==============================] - 1812s 75ms/step - loss: 0.0202 - acc: 0.9939\n",
      "Epoch 6/10\n",
      "24000/24000 [==============================] - 1812s 75ms/step - loss: 0.0177 - acc: 0.9948\n",
      "Epoch 7/10\n",
      "24000/24000 [==============================] - 1805s 75ms/step - loss: 0.0180 - acc: 0.9953\n",
      "Epoch 8/10\n",
      "24000/24000 [==============================] - 1792s 75ms/step - loss: 0.0151 - acc: 0.9965\n",
      "Epoch 9/10\n",
      "24000/24000 [==============================] - 1841s 77ms/step - loss: 0.0095 - acc: 0.9982\n",
      "Epoch 10/10\n",
      "24000/24000 [==============================] - 1930s 80ms/step - loss: 0.0198 - acc: 0.9948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21066bde0c8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model fitting on Training data\n",
    "model.fit(xTrain,yTrain,batch_size=32,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 1s 15ms/step\n",
      "Loss:  0.06790319502353669\n",
      "Accuracy:  97.99999904632568\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "[loss, accuracy] = model.evaluate(xTest,yTest,verbose=1)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
